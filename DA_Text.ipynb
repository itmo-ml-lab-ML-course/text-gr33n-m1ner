{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Парсер"
   ],
   "metadata": {
    "id": "-5w5mc7tt1Cl"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml.html as l\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "id": "lq3eAa4Lutwy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97cf0175-cc15-4f93-d13a-39569ec4dd19",
    "ExecuteTime": {
     "end_time": "2024-03-13T15:31:06.102086500Z",
     "start_time": "2024-03-13T15:31:01.548901500Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BROWSE_LINK = 'https://www.banki.ru/services/responses/bank/mts-bank/?page='\n",
    "WEBSITE = 'https://www.banki.ru'\n",
    "\n",
    "HEADER = {\n",
    "    'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' ,\n",
    "    'Accept-Encoding' : 'gzip, deflate, br' ,\n",
    "    'Accept-Language': 'en-US,en;q=0.9' ,\n",
    "    'Sec-Ch-Ua' : '\"Not A(Brand\";v=\"99\", \"Opera\";v=\"107\", \"Chromium\";v=\"121\"' ,\n",
    "    'Sec-Ch-Ua-Mobile': '?0' ,\n",
    "    'Sec-Ch-Ua-Platform': \"Windows\" ,\n",
    "    'Sec-Fetch-Dest': 'document' ,\n",
    "    'Sec-Fetch-Mode': 'navigate' ,\n",
    "    'Sec-Fetch-Site': 'same-origin' ,\n",
    "    'Sec-Fetch-User': '?1' ,\n",
    "    'Upgrade-Insecure-Requests': '1' ,\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 OPR/107.0.0.0' ,\n",
    "}\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "def clone(element):\n",
    "    return l.fromstring(l.tostring(element))\n",
    "\n",
    "def parse():\n",
    "    data = []\n",
    "    page = 1\n",
    "    while len(data) < SAMPLE_SIZE:\n",
    "        current_link = BROWSE_LINK + str(page) + \"&is_countable=on\"\n",
    "        request = requests.get(current_link, headers = HEADER)\n",
    "        document = l.fromstring(request.text)\n",
    "        for card in document.find_class(\"la8a5ef73\"):\n",
    "            card = clone(card)\n",
    "            links = card.xpath('//a/@href')\n",
    "            if links[0].find(\"/services/responses/bank/response\") != -1:\n",
    "                instance = parse_instance(links[0])\n",
    "                data.append(instance)\n",
    "            if len(data) >= SAMPLE_SIZE:\n",
    "                break\n",
    "        page += 1\n",
    "        print(\"len: \" + str(len(data)))\n",
    "    return data\n",
    "\n",
    "def parse_instance(link):\n",
    "    instance = []\n",
    "    link = WEBSITE + link\n",
    "    request = requests.get(link, headers=HEADER)\n",
    "    document = l.fromstring(request.text)\n",
    "\n",
    "    text = document.find_class(\"lb1789875 markdown-inside markdown-inside--list-type_circle-fill\")[0].text_content()\n",
    "    instance.append(text)\n",
    "    score = document.find_class(\"rating-grade\")[0].text_content();\n",
    "    instance.append(score)\n",
    "    return instance\n",
    "\n",
    "def save_data():\n",
    "    data = parse()\n",
    "    df = pd.DataFrame(data, columns=['Review', 'Score'])\n",
    "    df.to_csv('text_data.csv', index=False)\n",
    "\n",
    "save_data()"
   ],
   "metadata": {
    "id": "4BaIHyLOCeJB"
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "STOPWORDS = stopwords.words('russian')\n",
    "MYSTEM = Mystem()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def text_words(text):\n",
    "    return [word for word in MYSTEM.lemmatize(text) if not word.isspace() and not word in STOPWORDS]\n",
    "\n",
    "def process_texts(texts):\n",
    "    rus_alph = set(list('йцукенгшщзхъфывапролджэёячсмитьбю'))\n",
    "    drop_not_letters = lambda s: ''.join(list(filter(lambda c: c in rus_alph or c == ' ', s)))\n",
    "    texts = [drop_not_letters(comm.lower().replace(\"\\n\", \" \").replace(\".\", \" \").replace(\",\", \" \")) for comm in texts]\n",
    "    texts = list(map(text_words, texts))\n",
    "    return texts\n",
    "\n",
    "def count_all_words(texts):\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(text)\n",
    "    counts = Counter(all_words)\n",
    "    return counts\n",
    "\n",
    "def unwrap(counter):\n",
    "    return list(counter.keys()), list(counter.values())\n",
    "\n",
    "def embeddings_vocab(words):\n",
    "    return {words[i] : i for i in range(len(words))}\n",
    "\n",
    "def create_embeddings(texts_, vocabulary):\n",
    "    texts = [text for text in texts_]\n",
    "    length = max(list(map(len, texts)))\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = [vocabulary.get(word) for word in texts[i]]\n",
    "        texts[i].extend([len(vocabulary) for _ in range(length - len(texts[i]))])\n",
    "    return texts\n",
    "\n",
    "def process_score(score):\n",
    "    def binary(val):\n",
    "        if val <= 3:\n",
    "            return 0\n",
    "        return 1\n",
    "    return list(map(binary, score))\n",
    "\n",
    "def fix_data(texts, score):\n",
    "    processed_texts = process_texts(texts)\n",
    "    processed_score = process_score(score)\n",
    "    skip_long = [(processed_texts[i], processed_score[i]) for i in range(len(texts)) if len(processed_texts[i]) < 180]\n",
    "    return [x[0] for x in skip_long], [x[1] for x in skip_long]\n"
   ],
   "metadata": {
    "id": "zdCcvo_1zCkn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84c573cf-05a8-428c-837b-608117340dce",
    "ExecuteTime": {
     "end_time": "2024-03-13T15:31:12.396684800Z",
     "start_time": "2024-03-13T15:31:12.376339800Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DATA = pd.read_csv(\"text_data.csv\")\n",
    "TEXTS = DATA[\"Review\"].tolist()\n",
    "SCORE = DATA[\"Score\"].tolist()\n",
    "\n",
    "NEW_TEXTS, NEW_SCORE = fix_data(TEXTS, SCORE)\n",
    "\n",
    "[WORDS, COUNTS] = unwrap(count_all_words(NEW_TEXTS))\n",
    "\n",
    "VOCABULARY = embeddings_vocab(WORDS)\n",
    "EMBEDDINGS = create_embeddings(NEW_TEXTS, VOCABULARY)\n",
    "VOCABULARY_SIZE = len(WORDS) + 1"
   ],
   "metadata": {
    "id": "3M0g6bbct8Oa",
    "ExecuteTime": {
     "end_time": "2024-03-13T15:41:29.855768700Z",
     "start_time": "2024-03-13T15:31:22.046918800Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, score):\n",
    "        self.texts = texts\n",
    "        self.score = score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.texts[index]), self.score[index]\n",
    "\n",
    "\n",
    "class ScoreClassification(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embeddings_dim, hidden_dim, classes_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocabulary_size, embeddings_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embeddings_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, classes_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        x = self.linear(hidden[-1])\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, epochs, lr):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_train_loss = total_loss / len(train_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Train Loss: {average_train_loss}')\n",
    "\n",
    "texts_train, texts_test, score_train, score_test = train_test_split(EMBEDDINGS, NEW_SCORE, test_size=0.2, random_state=1)\n",
    "train_dataset = CustomDataset(texts_train, score_train)\n",
    "test_dataset = CustomDataset(texts_test, score_test)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "CLASSES_SIZE = 2\n",
    "\n",
    "MODEL = ScoreClassification(VOCABULARY_SIZE, len(EMBEDDINGS), HIDDEN_DIM, CLASSES_SIZE)\n",
    "train(MODEL, epochs=200, lr=0.05)"
   ],
   "metadata": {
    "id": "tn2hARNDNGTw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6639a29b-0f98-4eee-e8cb-cae302990bcb",
    "ExecuteTime": {
     "end_time": "2024-03-13T16:54:17.025066300Z",
     "start_time": "2024-03-13T15:53:43.611696500Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Train Loss: 0.47334613899389905\n",
      "Epoch 20\n",
      "Train Loss: 0.3916773498058319\n",
      "Epoch 30\n",
      "Train Loss: 0.3460238426923752\n",
      "Epoch 40\n",
      "Train Loss: 0.36917820076147717\n",
      "Epoch 50\n",
      "Train Loss: 0.318291408320268\n",
      "Epoch 60\n",
      "Train Loss: 0.3186407958467801\n",
      "Epoch 70\n",
      "Train Loss: 0.2930892010529836\n",
      "Epoch 80\n",
      "Train Loss: 0.2683047999938329\n",
      "Epoch 90\n",
      "Train Loss: 0.27024005353450775\n",
      "Epoch 100\n",
      "Train Loss: 0.24579465637604395\n",
      "Epoch 110\n",
      "Train Loss: 0.2762375424305598\n",
      "Epoch 120\n",
      "Train Loss: 0.22375448048114777\n",
      "Epoch 130\n",
      "Train Loss: 0.20041662951310477\n",
      "Epoch 140\n",
      "Train Loss: 0.23873620728651682\n",
      "Epoch 150\n",
      "Train Loss: 0.22707791378100714\n",
      "Epoch 160\n",
      "Train Loss: 0.31766921033461887\n",
      "Epoch 170\n",
      "Train Loss: 0.199457714955012\n",
      "Epoch 180\n",
      "Train Loss: 0.15615902344385782\n",
      "Epoch 190\n",
      "Train Loss: 0.12730027735233307\n",
      "Epoch 200\n",
      "Train Loss: 0.132468710343043\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSNxsTWwVppb",
    "outputId": "254c28e5-5df6-4ae3-d20d-11768ae5da73",
    "ExecuteTime": {
     "end_time": "2024-03-13T17:01:13.203804600Z",
     "start_time": "2024-03-13T17:01:13.184695500Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "ScoreClassification(\n  (embeddings): Embedding(5268, 937, padding_idx=0)\n  (lstm): LSTM(937, 300, batch_first=True)\n  (linear): Linear(in_features=300, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_f_score = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, y = data\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += accuracy_score(y, predicted)\n",
    "            correct_f_score += f1_score(y, predicted, average='weighted')\n",
    "            \n",
    "    print(f'Accuracy of the model: {100 * correct / total}%')\n",
    "    print(f'F-score of the model: {100 * correct_f_score / total}%')\n",
    "\n",
    "print(\"Test\")\n",
    "test_model(MODEL, test_loader)\n",
    "print(\"Train\")\n",
    "test_model(MODEL, train_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzD4cyGSe6Zz",
    "outputId": "d5b4bec5-a381-4e6f-faec-a4d28da6a6e7",
    "ExecuteTime": {
     "end_time": "2024-03-13T17:07:38.958602700Z",
     "start_time": "2024-03-13T17:07:37.322037100Z"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Accuracy of the model: 0.8837544326241136%\n",
      "F-score of the model: 0.8795065011820331%\n",
      "Train\n",
      "Accuracy of the model: 0.7624271199519849%\n",
      "F-score of the model: 0.7617321735221108%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T17:06:28.830771500Z",
     "start_time": "2024-03-13T17:06:28.815507Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
